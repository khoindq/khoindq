# Hey, I'm Khoi ðŸ‘‹

Big Data Engineer specializing in Apache Spark and distributed systems.

## What I Work With

**Big Data Stack**
Apache Spark â€¢ Hadoop â€¢ Kafka â€¢ Airflow

**Languages**
Python â€¢ Scala â€¢ Go â€¢ SQL

**Cloud & Tools**
AWS (EMR, Glue, S3, Redshift) â€¢ Docker â€¢ Kubernetes

**Databases**
PostgreSQL â€¢ Redis â€¢ Elasticsearch

## What I Do

I build and optimize data pipelines that process large-scale data. Most of my work involves:

- Writing Spark jobs (PySpark and Spark SQL)
- Designing ETL/ELT pipelines
- Optimizing query performance and reducing costs
- Working with streaming data (Kafka, Spark Streaming)
- Building data lakes and warehouses on AWS

## Recent Interests

- Delta Lake and lakehouse architectures
- Spark performance tuning at scale
- Modern data stack tools (dbt, Airbyte)

## Tech I Use Daily

```python
stack = {
    'processing': ['Spark', 'Pandas', 'Dask'],
    'orchestration': ['Airflow', 'Step Functions'],
    'storage': ['S3', 'Parquet', 'Delta Lake'],
    'streaming': ['Kafka', 'Kinesis'],
    'warehouse': ['Redshift', 'Athena']
}
```

---

Feel free to reach out if you want to discuss big data architecture or Spark optimization.
